{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37HCz2qyQGRZ",
        "outputId": "b8e31794-4fa6-49a2-f5f0-c186fb98827a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting tensorflow\n",
            "  Using cached tensorflow-2.18.0-cp311-cp311-macosx_12_0_arm64.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: keras in /opt/homebrew/lib/python3.11/site-packages (3.7.0)\n",
            "Collecting sentence-transformers\n",
            "  Using cached sentence_transformers-3.3.1-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /opt/homebrew/lib/python3.11/site-packages (from tensorflow) (2.1.0)\n",
            "Collecting astunparse>=1.6.0 (from tensorflow)\n",
            "  Using cached astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /opt/homebrew/lib/python3.11/site-packages (from tensorflow) (24.3.25)\n",
            "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow)\n",
            "  Using cached gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting google-pasta>=0.1.1 (from tensorflow)\n",
            "  Using cached google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /opt/homebrew/lib/python3.11/site-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/homebrew/lib/python3.11/site-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /opt/homebrew/lib/python3.11/site-packages (from tensorflow) (23.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /opt/homebrew/lib/python3.11/site-packages (from tensorflow) (5.29.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /opt/homebrew/lib/python3.11/site-packages (from tensorflow) (2.31.0)\n",
            "Requirement already satisfied: setuptools in /opt/homebrew/lib/python3.11/site-packages (from tensorflow) (74.1.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /opt/homebrew/lib/python3.11/site-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /opt/homebrew/lib/python3.11/site-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/homebrew/lib/python3.11/site-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /opt/homebrew/lib/python3.11/site-packages (from tensorflow) (1.16.0)\n",
            "Collecting grpcio<2.0,>=1.24.3 (from tensorflow)\n",
            "  Using cached grpcio-1.68.1-cp311-cp311-macosx_10_9_universal2.whl.metadata (3.9 kB)\n",
            "Collecting tensorboard<2.19,>=2.18 (from tensorflow)\n",
            "  Using cached tensorboard-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting numpy<2.1.0,>=1.26.0 (from tensorflow)\n",
            "  Using cached numpy-2.0.2-cp311-cp311-macosx_14_0_arm64.whl.metadata (60 kB)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /opt/homebrew/lib/python3.11/site-packages (from tensorflow) (3.12.1)\n",
            "Collecting ml-dtypes<0.5.0,>=0.4.0 (from tensorflow)\n",
            "  Using cached ml_dtypes-0.4.1-cp311-cp311-macosx_10_9_universal2.whl.metadata (20 kB)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/homebrew/lib/python3.11/site-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: rich in /opt/homebrew/lib/python3.11/site-packages (from keras) (13.7.1)\n",
            "Requirement already satisfied: namex in /opt/homebrew/lib/python3.11/site-packages (from keras) (0.0.8)\n",
            "Requirement already satisfied: optree in /opt/homebrew/lib/python3.11/site-packages (from keras) (0.13.1)\n",
            "Collecting transformers<5.0.0,>=4.41.0 (from sentence-transformers)\n",
            "  Using cached transformers-4.47.0-py3-none-any.whl.metadata (43 kB)\n",
            "Requirement already satisfied: tqdm in /opt/homebrew/lib/python3.11/site-packages (from sentence-transformers) (4.66.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /opt/homebrew/lib/python3.11/site-packages (from sentence-transformers) (2.2.1)\n",
            "Requirement already satisfied: scikit-learn in /opt/homebrew/lib/python3.11/site-packages (from sentence-transformers) (1.4.1.post1)\n",
            "Requirement already satisfied: scipy in /opt/homebrew/lib/python3.11/site-packages (from sentence-transformers) (1.12.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /opt/homebrew/lib/python3.11/site-packages (from sentence-transformers) (0.21.4)\n",
            "Requirement already satisfied: Pillow in /opt/homebrew/lib/python3.11/site-packages (from sentence-transformers) (10.2.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/homebrew/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
            "Requirement already satisfied: filelock in /opt/homebrew/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /opt/homebrew/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.2.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /opt/homebrew/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2023.11.17)\n",
            "Collecting markdown>=2.6.8 (from tensorboard<2.19,>=2.18->tensorflow)\n",
            "  Using cached Markdown-3.7-py3-none-any.whl.metadata (7.0 kB)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/homebrew/lib/python3.11/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /opt/homebrew/lib/python3.11/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: sympy in /opt/homebrew/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /opt/homebrew/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /opt/homebrew/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.3)\n",
            "Collecting huggingface-hub>=0.20.0 (from sentence-transformers)\n",
            "  Using cached huggingface_hub-0.26.5-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /opt/homebrew/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2023.12.25)\n",
            "Collecting tokenizers<0.22,>=0.21 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
            "  Using cached tokenizers-0.21.0-cp39-abi3-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /opt/homebrew/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/homebrew/lib/python3.11/site-packages (from rich->keras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/divyareddy/Library/Python/3.11/lib/python/site-packages (from rich->keras) (2.17.2)\n",
            "Collecting numpy<2.1.0,>=1.26.0 (from tensorflow)\n",
            "  Using cached numpy-1.26.4-cp311-cp311-macosx_11_0_arm64.whl.metadata (114 kB)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /opt/homebrew/lib/python3.11/site-packages (from scikit-learn->sentence-transformers) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/homebrew/lib/python3.11/site-packages (from scikit-learn->sentence-transformers) (3.4.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /opt/homebrew/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/homebrew/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /opt/homebrew/lib/python3.11/site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Using cached tensorflow-2.18.0-cp311-cp311-macosx_12_0_arm64.whl (239.5 MB)\n",
            "Using cached sentence_transformers-3.3.1-py3-none-any.whl (268 kB)\n",
            "Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
            "Using cached gast-0.6.0-py3-none-any.whl (21 kB)\n",
            "Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
            "Using cached grpcio-1.68.1-cp311-cp311-macosx_10_9_universal2.whl (11.2 MB)\n",
            "Using cached ml_dtypes-0.4.1-cp311-cp311-macosx_10_9_universal2.whl (397 kB)\n",
            "Using cached tensorboard-2.18.0-py3-none-any.whl (5.5 MB)\n",
            "Using cached transformers-4.47.0-py3-none-any.whl (10.1 MB)\n",
            "Using cached huggingface_hub-0.26.5-py3-none-any.whl (447 kB)\n",
            "Using cached numpy-1.26.4-cp311-cp311-macosx_11_0_arm64.whl (14.0 MB)\n",
            "Using cached Markdown-3.7-py3-none-any.whl (106 kB)\n",
            "Using cached tokenizers-0.21.0-cp39-abi3-macosx_11_0_arm64.whl (2.6 MB)\n",
            "Installing collected packages: numpy, markdown, grpcio, google-pasta, gast, astunparse, tensorboard, ml-dtypes, huggingface-hub, tokenizers, transformers, tensorflow, sentence-transformers\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.1.2\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1muninstall-no-record-file\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Cannot uninstall numpy 2.1.2\n",
            "\u001b[31m╰─>\u001b[0m The package's contents are unknown: no RECORD file was found for numpy.\n",
            "\n",
            "\u001b[1;36mhint\u001b[0m: The package was installed by brew. You should check if it can uninstall the package.\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow keras sentence-transformers keras-tuner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1LmQSaksQGRb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/m9/4j5hjgcd5qngvl020z555w240000gn/T/ipykernel_69602/3529243669.py:12: DeprecationWarning: `import kerastuner` is deprecated, please use `import keras_tuner`.\n",
            "  from kerastuner.tuners import Hyperband\n",
            "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.metrics import Precision, Recall, AUC\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from kerastuner.tuners import Hyperband\n",
        "# from tensorflow.keras.tuner import Hyperband  # Use Keras Tuner from TensorFlow\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "qsmTvSCFlxtE"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "a6YJg7wkQGRb"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('huggingface_with_perplexity.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pl8FUVblQGRb",
        "outputId": "d3e4825c-ef2e-4621-d630-50b67e992a50"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Count of NaN values per column:\n",
            "Series([], dtype: int64)\n"
          ]
        }
      ],
      "source": [
        "# Check the total number of NaNs in each column\n",
        "nan_summary = df.isna().sum()\n",
        "print(\"\\nCount of NaN values per column:\")\n",
        "print(nan_summary[nan_summary > 0])  # Show only columns with NaN values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "DnFlesTjQGRb"
      },
      "outputs": [],
      "source": [
        "df = df[~df['perplexity'].isna()]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "dcY5mxxVQGRb"
      },
      "outputs": [],
      "source": [
        "# Identify rows where 'generated' == 'generated'\n",
        "rows_to_drop = df[df['generated'] == 'generated'].index\n",
        "\n",
        "# Drop these rows\n",
        "dataset = df.drop(rows_to_drop)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wHcoSUGCCiNZ",
        "outputId": "051318ff-d7e8-4b44-e36b-0d570b1eda91"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10380\n",
            "(4620, 16)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Extract the \"text\" column for similarity checking\n",
        "texts = dataset[\"text\"].tolist()\n",
        "\n",
        "# Compute TF-IDF vectors for the text data\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = vectorizer.fit_transform(texts)\n",
        "\n",
        "# Compute cosine similarity matrix\n",
        "cosine_sim_matrix = cosine_similarity(tfidf_matrix)\n",
        "\n",
        "# Remove duplicate-like rows based on a 90% similarity threshold\n",
        "to_remove = set()\n",
        "for i in range(len(texts)):\n",
        "    for j in range(i + 1, len(texts)):\n",
        "        if cosine_sim_matrix[i, j] > 0.9:  # 90% similarity threshold\n",
        "            to_remove.add(j)\n",
        "\n",
        "print(len(to_remove))\n",
        "df_distinct = dataset.drop(list(to_remove)).reset_index(drop=True)\n",
        "df_distinct = dataset.drop(list(to_remove)).reset_index(drop=True)\n",
        "print(df_distinct.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nUaxLCdTQGRc",
        "outputId": "1a2e324d-6cb3-4be5-e774-0009c985d8b8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(4620, 16)"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_distinct.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bobKl6xzivAr",
        "outputId": "eff996d1-1221-4c2c-b7af-9b8f49d25d2a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Is GPU available? False\n"
          ]
        }
      ],
      "source": [
        "print(\"Is GPU available?\", torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "f2f1b1b98473413c989897959b46daa8",
            "254dec6487f74f80bcbd408738a89c4e",
            "474a926bd1cf48f5b2f27bb29d6a6813",
            "e98152c037584a72987b6d408eadb872",
            "c8d34eee2bc94b149910e202f48d87c5",
            "12f0d00ce5564f78970f887ea5255c8a",
            "0b63b45636644e75803e0ddaacf1a00f",
            "da60e7fbda044daf95d61e859a946e71",
            "342d7d6da7a64de0a5e8ef3bc4b9d4d1",
            "83e96cd4380d4f3cb43d1ef435320e17",
            "e14f11dbefd64d62b08b8ffb3ff4c0f3"
          ]
        },
        "id": "7qDe7bG1UMa4",
        "outputId": "6fd2be76-0e9d-45a8-f753-715a947abb10"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Batches: 100%|██████████| 145/145 [00:41<00:00,  3.48it/s]\n"
          ]
        }
      ],
      "source": [
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "text_embeddings = model.encode(df_distinct['text'].tolist(), show_progress_bar=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ys0hVZlaQGRc",
        "outputId": "45ebd148-d365-4c81-f9de-3fb36d6a893b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[-0.01686272 -0.01975334  0.00672336 ... -0.03179123  0.05161779\n",
            "   0.04418319]\n",
            " [ 0.03201037  0.02006243  0.07152901 ...  0.03716383 -0.05485476\n",
            "   0.06056327]\n",
            " [-0.01988794  0.09259275  0.08762163 ...  0.01249938 -0.05002529\n",
            "   0.02056968]\n",
            " ...\n",
            " [ 0.04894435 -0.04333448  0.02027768 ...  0.07240842 -0.07655276\n",
            "  -0.06850141]\n",
            " [ 0.00574559 -0.04813547  0.08095636 ...  0.03290918 -0.00781281\n",
            "  -0.03662317]\n",
            " [ 0.06045514 -0.04425557 -0.0092147  ...  0.02052861  0.08309416\n",
            "   0.08355667]]\n"
          ]
        }
      ],
      "source": [
        "print(text_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ildoAffC52Et"
      },
      "outputs": [],
      "source": [
        "embeddings_df = pd.DataFrame(text_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YGN6J2E-59FM",
        "outputId": "b2e3f83d-7e51-435b-d6b4-9d1d1baae148"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(15000, 384)\n"
          ]
        }
      ],
      "source": [
        "print(embeddings_df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "blpf3cYkQGRc"
      },
      "outputs": [],
      "source": [
        "X = embeddings_df\n",
        "y = df_distinct['generated']\n",
        "y.astype(int)  # Target column\n",
        "X = np.array(X, dtype=np.float32)  # Convert X to float32\n",
        "y = np.array(y, dtype=np.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "MI5SwUrzQGRc"
      },
      "outputs": [],
      "source": [
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "id": "THx2QyLJGHNx",
        "outputId": "fe20ac88-3f48-4962-9e86-bef7ad8295b7"
      },
      "outputs": [],
      "source": [
        "assert not any(np.array_equal(X_train[i], X_test[j]) for i in range(len(X_train)) for j in range(len(X_test))), \"Data Leakage Detected!\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "id": "XavHWZxkGiv1",
        "outputId": "76371752-1d5c-4a2f-9a7e-ed431bf4e618"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of overlapping samples: 0\n"
          ]
        }
      ],
      "source": [
        "# Find duplicates between X_train and X_test\n",
        "duplicates = [np.array_equal(X_train[i], X_test[j]) for i in range(len(X_train)) for j in range(len(X_test))]\n",
        "\n",
        "# Extract overlapping samples (optional for debugging)\n",
        "overlapping_indices = [\n",
        "    (i, j) for i in range(len(X_train)) for j in range(len(X_test)) if np.array_equal(X_train[i], X_test[j])\n",
        "]\n",
        "\n",
        "print(f\"Number of overlapping samples: {len(overlapping_indices)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6xhTOGeJog8d",
        "outputId": "cab51821-d51e-49f9-d8ac-6a20b8d3cc45"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training set size: 2772\n",
            "Validation set size: 924\n",
            "Test set size: 924\n"
          ]
        }
      ],
      "source": [
        "print(f\"Training set size: {X_train.shape[0]}\")\n",
        "print(f\"Validation set size: {X_val.shape[0]}\")\n",
        "print(f\"Test set size: {X_test.shape[0]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "082dq_E3QGRc"
      },
      "outputs": [],
      "source": [
        "def build_model(hp):\n",
        "    model = Sequential([\n",
        "        Input(shape=(X.shape[1],)),  # Input shape based on combined feature vector\n",
        "        Dense(hp.Int('units_1', 32, 128, step=32), activation='relu', kernel_regularizer=l2(1e-4)),\n",
        "        Dropout(hp.Float('dropout_1', 0.2, 0.6, step=0.1)),\n",
        "        Dense(hp.Int('units_2', 16, 64, step=16), activation='relu', kernel_regularizer=l2(1e-4)),\n",
        "        Dropout(hp.Float('dropout_2', 0.2, 0.6, step=0.1)),\n",
        "        Dense(1, activation='sigmoid')  # Binary classification\n",
        "    ])\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=hp.Choice('learning_rate', [1e-4, 1e-5, 1e-6])),\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy',Precision(), Recall(), AUC()]\n",
        "    )\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AgZYZpsopTLB",
        "outputId": "9004aa43-3fcd-4e08-c366-1198eb7a0115"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "# Clear any\n",
        "if os.path.exists('mlp_embed_tuning'):\n",
        "    shutil.rmtree('mlp_embed_tuning')\n",
        "\n",
        "\n",
        "tuner = Hyperband(\n",
        "    build_model,\n",
        "    objective='val_accuracy',\n",
        "    max_epochs=20,\n",
        "    factor=2,\n",
        "    directory='mlp_embed_tuning',\n",
        "    project_name='mlp_tune_embeddings'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "DTNXY_hwQGRd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Search: Running Trial #1\n",
            "\n",
            "Value             |Best Value So Far |Hyperparameter\n",
            "64                |64                |units_1\n",
            "0.4               |0.4               |dropout_1\n",
            "32                |32                |units_2\n",
            "0.3               |0.3               |dropout_2\n",
            "1e-06             |1e-06             |learning_rate\n",
            "2                 |2                 |tuner/epochs\n",
            "0                 |0                 |tuner/initial_epoch\n",
            "4                 |4                 |tuner/bracket\n",
            "0                 |0                 |tuner/round\n",
            "\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m/Users/divyareddy/Documents/CS 6140 ML/Project/detect-ai-text/MLP_embeddings.ipynb Cell 22\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/divyareddy/Documents/CS%206140%20ML/Project/detect-ai-text/MLP_embeddings.ipynb#X31sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m tuner\u001b[39m.\u001b[39;49msearch(X_train, y_train, validation_data\u001b[39m=\u001b[39;49m(X_val, y_val), epochs\u001b[39m=\u001b[39;49m\u001b[39m20\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m32\u001b[39;49m, verbose\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m)\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/keras_tuner/src/engine/base_tuner.py:234\u001b[0m, in \u001b[0;36mBaseTuner.search\u001b[0;34m(self, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m    233\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_trial_begin(trial)\n\u001b[0;32m--> 234\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_try_run_and_update_trial(trial, \u001b[39m*\u001b[39;49mfit_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_kwargs)\n\u001b[1;32m    235\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_trial_end(trial)\n\u001b[1;32m    236\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_search_end()\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/keras_tuner/src/engine/base_tuner.py:274\u001b[0m, in \u001b[0;36mBaseTuner._try_run_and_update_trial\u001b[0;34m(self, trial, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_try_run_and_update_trial\u001b[39m(\u001b[39mself\u001b[39m, trial, \u001b[39m*\u001b[39mfit_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_kwargs):\n\u001b[1;32m    273\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 274\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_and_update_trial(trial, \u001b[39m*\u001b[39;49mfit_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_kwargs)\n\u001b[1;32m    275\u001b[0m         trial\u001b[39m.\u001b[39mstatus \u001b[39m=\u001b[39m trial_module\u001b[39m.\u001b[39mTrialStatus\u001b[39m.\u001b[39mCOMPLETED\n\u001b[1;32m    276\u001b[0m         \u001b[39mreturn\u001b[39;00m\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/keras_tuner/src/engine/base_tuner.py:239\u001b[0m, in \u001b[0;36mBaseTuner._run_and_update_trial\u001b[0;34m(self, trial, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_and_update_trial\u001b[39m(\u001b[39mself\u001b[39m, trial, \u001b[39m*\u001b[39mfit_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_kwargs):\n\u001b[0;32m--> 239\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun_trial(trial, \u001b[39m*\u001b[39;49mfit_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_kwargs)\n\u001b[1;32m    240\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moracle\u001b[39m.\u001b[39mget_trial(trial\u001b[39m.\u001b[39mtrial_id)\u001b[39m.\u001b[39mmetrics\u001b[39m.\u001b[39mexists(\n\u001b[1;32m    241\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moracle\u001b[39m.\u001b[39mobjective\u001b[39m.\u001b[39mname\n\u001b[1;32m    242\u001b[0m     ):\n\u001b[1;32m    243\u001b[0m         \u001b[39m# The oracle is updated by calling `self.oracle.update_trial()` in\u001b[39;00m\n\u001b[1;32m    244\u001b[0m         \u001b[39m# `Tuner.run_trial()`. For backward compatibility, we support this\u001b[39;00m\n\u001b[1;32m    245\u001b[0m         \u001b[39m# use case. No further action needed in this case.\u001b[39;00m\n\u001b[1;32m    246\u001b[0m         warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    247\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mThe use case of calling \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    248\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m`self.oracle.update_trial(trial_id, metrics)` \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    254\u001b[0m             stacklevel\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,\n\u001b[1;32m    255\u001b[0m         )\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/keras_tuner/src/tuners/hyperband.py:427\u001b[0m, in \u001b[0;36mHyperband.run_trial\u001b[0;34m(self, trial, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    425\u001b[0m     fit_kwargs[\u001b[39m\"\u001b[39m\u001b[39mepochs\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m hp\u001b[39m.\u001b[39mvalues[\u001b[39m\"\u001b[39m\u001b[39mtuner/epochs\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    426\u001b[0m     fit_kwargs[\u001b[39m\"\u001b[39m\u001b[39minitial_epoch\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m hp\u001b[39m.\u001b[39mvalues[\u001b[39m\"\u001b[39m\u001b[39mtuner/initial_epoch\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m--> 427\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mrun_trial(trial, \u001b[39m*\u001b[39;49mfit_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_kwargs)\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/keras_tuner/src/engine/tuner.py:314\u001b[0m, in \u001b[0;36mTuner.run_trial\u001b[0;34m(self, trial, *args, **kwargs)\u001b[0m\n\u001b[1;32m    312\u001b[0m     callbacks\u001b[39m.\u001b[39mappend(model_checkpoint)\n\u001b[1;32m    313\u001b[0m     copied_kwargs[\u001b[39m\"\u001b[39m\u001b[39mcallbacks\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m callbacks\n\u001b[0;32m--> 314\u001b[0m     obj_value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_build_and_fit_model(trial, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcopied_kwargs)\n\u001b[1;32m    316\u001b[0m     histories\u001b[39m.\u001b[39mappend(obj_value)\n\u001b[1;32m    317\u001b[0m \u001b[39mreturn\u001b[39;00m histories\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/keras_tuner/src/engine/tuner.py:232\u001b[0m, in \u001b[0;36mTuner._build_and_fit_model\u001b[0;34m(self, trial, *args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"For AutoKeras to override.\u001b[39;00m\n\u001b[1;32m    215\u001b[0m \n\u001b[1;32m    216\u001b[0m \u001b[39mDO NOT REMOVE this function. AutoKeras overrides the function to tune\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[39m    The fit history.\u001b[39;00m\n\u001b[1;32m    230\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    231\u001b[0m hp \u001b[39m=\u001b[39m trial\u001b[39m.\u001b[39mhyperparameters\n\u001b[0;32m--> 232\u001b[0m model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_try_build(hp)\n\u001b[1;32m    233\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhypermodel\u001b[39m.\u001b[39mfit(hp, model, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    235\u001b[0m \u001b[39m# Save the build config for model loading later.\u001b[39;00m\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/keras_tuner/src/engine/tuner.py:161\u001b[0m, in \u001b[0;36mTuner._try_build\u001b[0;34m(self, hp)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_try_build\u001b[39m(\u001b[39mself\u001b[39m, hp):\n\u001b[1;32m    160\u001b[0m     \u001b[39m# clean-up TF graph from previously stored (defunct) graph\u001b[39;00m\n\u001b[0;32m--> 161\u001b[0m     keras\u001b[39m.\u001b[39;49mbackend\u001b[39m.\u001b[39;49mclear_session()\n\u001b[1;32m    162\u001b[0m     gc\u001b[39m.\u001b[39mcollect()\n\u001b[1;32m    164\u001b[0m     model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_hypermodel(hp)\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/keras/src/backend/common/global_state.py:88\u001b[0m, in \u001b[0;36mclear_session\u001b[0;34m(free_memory)\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[39mif\u001b[39;00m tf\u001b[39m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m     84\u001b[0m         \u001b[39m# Clear pending nodes in eager executors, kernel caches and\u001b[39;00m\n\u001b[1;32m     85\u001b[0m         \u001b[39m# step_containers.\u001b[39;00m\n\u001b[1;32m     86\u001b[0m         \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39meager\u001b[39;00m \u001b[39mimport\u001b[39;00m context\n\u001b[0;32m---> 88\u001b[0m         context\u001b[39m.\u001b[39;49mcontext()\u001b[39m.\u001b[39;49mclear_kernel_cache()\n\u001b[1;32m     89\u001b[0m \u001b[39melif\u001b[39;00m backend\u001b[39m.\u001b[39mbackend() \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtorch\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m     90\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_dynamo\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mdynamo\u001b[39;00m\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/tensorflow/python/eager/context.py:1012\u001b[0m, in \u001b[0;36mContext.clear_kernel_cache\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1010\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Clear kernel cache and reset all stateful kernels.\"\"\"\u001b[39;00m\n\u001b[1;32m   1011\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_context_handle \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1012\u001b[0m   pywrap_tfe\u001b[39m.\u001b[39;49mTFE_ContextClearCaches(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_context_handle)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "tuner.search(X_train, y_train, validation_data=(X_val, y_val), epochs=20, batch_size=32, verbose=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bA1gkKiZt70U",
        "outputId": "a5ac12d3-7ebb-4e8f-bc5b-3ccc22eaa2a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best hyperparameters: {'units_1': 48, 'dropout_1': 0.30000000000000004, 'units_2': 24, 'dropout_2': 0.2, 'learning_rate': 0.0001, 'tuner/epochs': 20, 'tuner/initial_epoch': 10, 'tuner/bracket': 4, 'tuner/round': 4, 'tuner/trial_id': '0035'}\n"
          ]
        }
      ],
      "source": [
        "best_hps = tuner.get_best_hyperparameters(num_trials=10)[0]\n",
        "print(f\"Best hyperparameters: {best_hps.values}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MSTtKKw5t_ts",
        "outputId": "2055c644-e85a-4bd1-9896-94d49db51778"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.4954 - auc_3: 0.4852 - loss: 0.7058 - precision_3: 0.4691 - recall_3: 0.1738 - val_accuracy: 0.5541 - val_auc_3: 0.6845 - val_loss: 0.6996 - val_precision_3: 0.8022 - val_recall_3: 0.1563\n",
            "Epoch 2/20\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5512 - auc_3: 0.6362 - loss: 0.6977 - precision_3: 0.6453 - recall_3: 0.2209 - val_accuracy: 0.5920 - val_auc_3: 0.8559 - val_loss: 0.6872 - val_precision_3: 0.9327 - val_recall_3: 0.2077\n",
            "Epoch 3/20\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.6098 - auc_3: 0.7827 - loss: 0.6829 - precision_3: 0.8640 - recall_3: 0.2421 - val_accuracy: 0.6483 - val_auc_3: 0.9207 - val_loss: 0.6660 - val_precision_3: 0.9671 - val_recall_3: 0.3148\n",
            "Epoch 4/20\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.6542 - auc_3: 0.8611 - loss: 0.6606 - precision_3: 0.8995 - recall_3: 0.3342 - val_accuracy: 0.7110 - val_auc_3: 0.9553 - val_loss: 0.6329 - val_precision_3: 0.9762 - val_recall_3: 0.4390\n",
            "Epoch 5/20\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7174 - auc_3: 0.9135 - loss: 0.6211 - precision_3: 0.9160 - recall_3: 0.4449 - val_accuracy: 0.8247 - val_auc_3: 0.9692 - val_loss: 0.5861 - val_precision_3: 0.9721 - val_recall_3: 0.6724\n",
            "Epoch 6/20\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8168 - auc_3: 0.9487 - loss: 0.5756 - precision_3: 0.9416 - recall_3: 0.6625 - val_accuracy: 0.8950 - val_auc_3: 0.9773 - val_loss: 0.5274 - val_precision_3: 0.9557 - val_recall_3: 0.8308\n",
            "Epoch 7/20\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8718 - auc_3: 0.9607 - loss: 0.5191 - precision_3: 0.9334 - recall_3: 0.7920 - val_accuracy: 0.9167 - val_auc_3: 0.9825 - val_loss: 0.4632 - val_precision_3: 0.9535 - val_recall_3: 0.8779\n",
            "Epoch 8/20\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8947 - auc_3: 0.9701 - loss: 0.4547 - precision_3: 0.9307 - recall_3: 0.8503 - val_accuracy: 0.9307 - val_auc_3: 0.9861 - val_loss: 0.3993 - val_precision_3: 0.9528 - val_recall_3: 0.9079\n",
            "Epoch 9/20\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9188 - auc_3: 0.9747 - loss: 0.3982 - precision_3: 0.9294 - recall_3: 0.9053 - val_accuracy: 0.9340 - val_auc_3: 0.9885 - val_loss: 0.3448 - val_precision_3: 0.9552 - val_recall_3: 0.9122\n",
            "Epoch 10/20\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9322 - auc_3: 0.9786 - loss: 0.3479 - precision_3: 0.9427 - recall_3: 0.9190 - val_accuracy: 0.9394 - val_auc_3: 0.9900 - val_loss: 0.2984 - val_precision_3: 0.9557 - val_recall_3: 0.9229\n",
            "Epoch 11/20\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9310 - auc_3: 0.9801 - loss: 0.3090 - precision_3: 0.9376 - recall_3: 0.9194 - val_accuracy: 0.9448 - val_auc_3: 0.9909 - val_loss: 0.2622 - val_precision_3: 0.9502 - val_recall_3: 0.9400\n",
            "Epoch 12/20\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9392 - auc_3: 0.9832 - loss: 0.2724 - precision_3: 0.9451 - recall_3: 0.9308 - val_accuracy: 0.9513 - val_auc_3: 0.9922 - val_loss: 0.2346 - val_precision_3: 0.9668 - val_recall_3: 0.9358\n",
            "Epoch 13/20\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9378 - auc_3: 0.9854 - loss: 0.2491 - precision_3: 0.9276 - recall_3: 0.9432 - val_accuracy: 0.9589 - val_auc_3: 0.9928 - val_loss: 0.2108 - val_precision_3: 0.9497 - val_recall_3: 0.9700\n",
            "Epoch 14/20\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9450 - auc_3: 0.9864 - loss: 0.2304 - precision_3: 0.9292 - recall_3: 0.9611 - val_accuracy: 0.9610 - val_auc_3: 0.9936 - val_loss: 0.1928 - val_precision_3: 0.9654 - val_recall_3: 0.9572\n",
            "Epoch 15/20\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9471 - auc_3: 0.9867 - loss: 0.2118 - precision_3: 0.9420 - recall_3: 0.9458 - val_accuracy: 0.9654 - val_auc_3: 0.9939 - val_loss: 0.1756 - val_precision_3: 0.9598 - val_recall_3: 0.9722\n",
            "Epoch 16/20\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9475 - auc_3: 0.9836 - loss: 0.2118 - precision_3: 0.9437 - recall_3: 0.9491 - val_accuracy: 0.9686 - val_auc_3: 0.9943 - val_loss: 0.1635 - val_precision_3: 0.9640 - val_recall_3: 0.9743\n",
            "Epoch 17/20\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9560 - auc_3: 0.9892 - loss: 0.1836 - precision_3: 0.9538 - recall_3: 0.9557 - val_accuracy: 0.9697 - val_auc_3: 0.9947 - val_loss: 0.1534 - val_precision_3: 0.9660 - val_recall_3: 0.9743\n",
            "Epoch 18/20\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9634 - auc_3: 0.9922 - loss: 0.1635 - precision_3: 0.9594 - recall_3: 0.9687 - val_accuracy: 0.9697 - val_auc_3: 0.9949 - val_loss: 0.1456 - val_precision_3: 0.9680 - val_recall_3: 0.9722\n",
            "Epoch 19/20\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9564 - auc_3: 0.9911 - loss: 0.1626 - precision_3: 0.9564 - recall_3: 0.9539 - val_accuracy: 0.9708 - val_auc_3: 0.9950 - val_loss: 0.1385 - val_precision_3: 0.9661 - val_recall_3: 0.9764\n",
            "Epoch 20/20\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9640 - auc_3: 0.9906 - loss: 0.1588 - precision_3: 0.9585 - recall_3: 0.9689 - val_accuracy: 0.9708 - val_auc_3: 0.9951 - val_loss: 0.1330 - val_precision_3: 0.9661 - val_recall_3: 0.9764\n"
          ]
        }
      ],
      "source": [
        "best_model = tuner.hypermodel.build(best_hps)\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "history = best_model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=20, batch_size=32, callbacks=[early_stopping])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tb6kfvcLJ2eC",
        "outputId": "769bca45-0ced-4a4c-e0a7-445b6da2f9f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Set Distribution: Counter({0.0: 1408, 1.0: 1364})\n",
            "Validation Set Distribution: Counter({1.0: 467, 0.0: 457})\n",
            "Test Set Distribution: Counter({1.0: 479, 0.0: 445})\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "\n",
        "print(\"Training Set Distribution:\", Counter(y_train))\n",
        "print(\"Validation Set Distribution:\", Counter(y_val))\n",
        "print(\"Test Set Distribution:\", Counter(y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w5-cLJRHuFL4",
        "outputId": "9f62fa46-1120-4143-c33f-685da07f9b48"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9825 - auc_7: 0.9989 - loss: 0.0693 - precision_7: 0.9869 - recall_7: 0.9785 \n",
            "Test Loss: 0.07109230011701584\n",
            "Test Accuracy: 0.9986770153045654\n",
            "Test Precision: 0.9894291758537292\n",
            "Test Recall: 0.9770354628562927\n",
            "Test AUC: 0.8252246975898743\n"
          ]
        }
      ],
      "source": [
        "loss, accuracy,precision, recall, accuracy  = best_model.evaluate(X_test, y_test)\n",
        "print(f\"Test Loss: {loss}\")\n",
        "print(f\"Test Accuracy: {accuracy}\")\n",
        "print(f\"Test Precision: {precision}\")\n",
        "print(f\"Test Recall: {recall}\")\n",
        "print(f\"Test AUC: {auc}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "LaoEyV34yS96"
      },
      "outputs": [],
      "source": [
        "df_htest = pd.read_csv('Dataset_with_new_features.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "RFic7uPtClcl"
      },
      "outputs": [],
      "source": [
        "# Identify rows where 'generated' == 'generated'\n",
        "rows_to_drop = df_htest[df_htest['generated'] == 'generated'].index\n",
        "\n",
        "# Drop these rows\n",
        "dataset_h = df_htest.drop(rows_to_drop)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PbFwxr7s9xSz",
        "outputId": "2f8805bc-af66-435f-eb39-9e10a2cc5b4e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "184\n",
            "(9426, 29)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Extract the \"text\" column for similarity checking\n",
        "texts = dataset_h[\"text\"].tolist()\n",
        "\n",
        "# Compute TF-IDF vectors for the text data\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = vectorizer.fit_transform(texts)\n",
        "\n",
        "# Compute cosine similarity matrix\n",
        "cosine_sim_matrix = cosine_similarity(tfidf_matrix)\n",
        "\n",
        "# Remove duplicate-like rows based on a 90% similarity threshold\n",
        "to_remove = set()\n",
        "for i in range(len(texts)):\n",
        "    for j in range(i + 1, len(texts)):\n",
        "        if cosine_sim_matrix[i, j] > 0.9:  # 90% similarity threshold\n",
        "            to_remove.add(j)\n",
        "\n",
        "print(len(to_remove))\n",
        "df_h_distinct = dataset_h.drop(list(to_remove)).reset_index(drop=True)\n",
        "df_h_distinct = dataset_h.drop(list(to_remove)).reset_index(drop=True)\n",
        "print(df_h_distinct.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "66oM-t5yIvks"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Batches: 100%|██████████| 295/295 [01:25<00:00,  3.47it/s]\n"
          ]
        }
      ],
      "source": [
        "text_embeddings_h = model.encode(df_h_distinct['text'].tolist(), show_progress_bar=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SpkrZ42sI4WF",
        "outputId": "11e32cc8-2a7a-4f2f-e225-5f6d5eeb8b77"
      },
      "outputs": [],
      "source": [
        "X_h = text_embeddings_h\n",
        "y_h = df_h_distinct['generated']\n",
        "y_h.astype(int)  # Target column\n",
        "X_h = np.array(X_h, dtype=np.float32)  # Convert X to float32\n",
        "y_h = np.array(y_h, dtype=np.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 356us/step\n",
            "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 292us/step\n",
            "[0.         0.         0.         ... 0.99981854 0.99981854 1.        ]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import roc_curve, auc, confusion_matrix, classification_report, precision_recall_curve\n",
        "y_pred = (best_model.predict(X_h) > 0.5).astype(int)  # Predicted labels\n",
        "y_prob = best_model.predict(X_h).ravel()  # Predicted probabilities\n",
        "fpr, tpr, thresholds = roc_curve(y_h, y_prob)\n",
        "print(fpr)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "balanced_accuracy = 0.5 * (tpr.mean() + (1 - fpr).mean())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ROC AUC: 0.8518019574458421\n",
            "Balanced Accuracy: 0.7080878929655267\n",
            "True Positive Rate: 0.7147056114729294\n",
            "False Positive Rate: 0.29852982554187607\n",
            "Thresholds: [          inf 9.9104941e-01 9.7957587e-01 ... 3.9524937e-04 3.3891349e-04\n",
            " 3.0215704e-04]\n"
          ]
        }
      ],
      "source": [
        "print(f\"ROC AUC: {roc_auc}\")\n",
        "print(f\"Balanced Accuracy: {balanced_accuracy}\")\n",
        "print(f\"True Positive Rate: {tpr.mean()}\")\n",
        "print(f\"False Positive Rate: {fpr.mean()}\")\n",
        "print(f\"Thresholds: {thresholds}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 541us/step - accuracy: 0.7885 - auc_3: 0.8515 - loss: 0.5233 - precision_3: 0.7625 - recall_3: 0.7614\n",
            "Test Loss: 0.5166563987731934\n",
            "Test Accuracy: 0.7908974885940552\n",
            "Test Precision: 0.7401185631752014\n",
            "Test Recall: 0.7652618288993835\n",
            "Test AUC: 0.8518397808074951\n"
          ]
        }
      ],
      "source": [
        "loss, accuracy,precision, recall, auc  = best_model.evaluate(X_h, y_h)\n",
        "print(f\"Test Loss: {loss}\")\n",
        "print(f\"Test Accuracy: {accuracy}\")\n",
        "print(f\"Test Precision: {precision}\")\n",
        "print(f\"Test Recall: {recall}\")\n",
        "print(f\"Test AUC: {auc}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_htest_2 = pd.read_csv('25k_huggingface.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Identify rows where 'generated' == 'generated'\n",
        "rows_to_drop = df_htest_2[df_htest_2['generated'] == 'generated'].index\n",
        "\n",
        "# Drop these rows\n",
        "dataset_h2 = df_htest_2.drop(rows_to_drop)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Batches: 100%|██████████| 744/744 [03:44<00:00,  3.32it/s]\n"
          ]
        }
      ],
      "source": [
        "text_embeddings_h2 = model.encode(dataset_h2['text'].tolist(), show_progress_bar=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_h2 = text_embeddings_h2\n",
        "y_h2 = df_h_distinct['generated']\n",
        "y_h2.astype(int)  # Target column\n",
        "X_h2 = np.array(X_h2, dtype=np.float32)  # Convert X to float32\n",
        "y_h2 = np.array(y_h2, dtype=np.float32)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    },
    "orig_nbformat": 4,
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0b63b45636644e75803e0ddaacf1a00f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "12f0d00ce5564f78970f887ea5255c8a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "254dec6487f74f80bcbd408738a89c4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_12f0d00ce5564f78970f887ea5255c8a",
            "placeholder": "​",
            "style": "IPY_MODEL_0b63b45636644e75803e0ddaacf1a00f",
            "value": "Batches: 100%"
          }
        },
        "342d7d6da7a64de0a5e8ef3bc4b9d4d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "474a926bd1cf48f5b2f27bb29d6a6813": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_da60e7fbda044daf95d61e859a946e71",
            "max": 145,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_342d7d6da7a64de0a5e8ef3bc4b9d4d1",
            "value": 145
          }
        },
        "83e96cd4380d4f3cb43d1ef435320e17": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c8d34eee2bc94b149910e202f48d87c5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "da60e7fbda044daf95d61e859a946e71": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e14f11dbefd64d62b08b8ffb3ff4c0f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e98152c037584a72987b6d408eadb872": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_83e96cd4380d4f3cb43d1ef435320e17",
            "placeholder": "​",
            "style": "IPY_MODEL_e14f11dbefd64d62b08b8ffb3ff4c0f3",
            "value": " 145/145 [00:14&lt;00:00, 18.81it/s]"
          }
        },
        "f2f1b1b98473413c989897959b46daa8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_254dec6487f74f80bcbd408738a89c4e",
              "IPY_MODEL_474a926bd1cf48f5b2f27bb29d6a6813",
              "IPY_MODEL_e98152c037584a72987b6d408eadb872"
            ],
            "layout": "IPY_MODEL_c8d34eee2bc94b149910e202f48d87c5"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
